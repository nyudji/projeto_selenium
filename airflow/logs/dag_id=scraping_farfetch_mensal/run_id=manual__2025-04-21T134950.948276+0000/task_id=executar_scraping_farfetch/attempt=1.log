[2025-04-21T13:52:40.546+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-21T13:52:40.559+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: scraping_farfetch_mensal.executar_scraping_farfetch manual__2025-04-21T13:49:50.948276+00:00 [queued]>
[2025-04-21T13:52:40.564+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: scraping_farfetch_mensal.executar_scraping_farfetch manual__2025-04-21T13:49:50.948276+00:00 [queued]>
[2025-04-21T13:52:40.565+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-21T13:52:40.578+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): executar_scraping_farfetch> on 2025-04-21 13:49:50.948276+00:00
[2025-04-21T13:52:40.585+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1347) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-21T13:52:40.586+0000] {standard_task_runner.py:72} INFO - Started process 1349 to run task
[2025-04-21T13:52:40.586+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'scraping_farfetch_mensal', 'executar_scraping_farfetch', 'manual__2025-04-21T13:49:50.948276+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/dag_scraping.py', '--cfg-path', '/tmp/tmp7ioke1p4']
[2025-04-21T13:52:40.588+0000] {standard_task_runner.py:105} INFO - Job 3: Subtask executar_scraping_farfetch
[2025-04-21T13:52:40.628+0000] {task_command.py:467} INFO - Running <TaskInstance: scraping_farfetch_mensal.executar_scraping_farfetch manual__2025-04-21T13:49:50.948276+00:00 [running]> on host 32047fb085ea
[2025-04-21T13:52:40.694+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='nicolas' AIRFLOW_CTX_DAG_ID='scraping_farfetch_mensal' AIRFLOW_CTX_TASK_ID='executar_scraping_farfetch' AIRFLOW_CTX_EXECUTION_DATE='2025-04-21T13:49:50.948276+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-04-21T13:49:50.948276+00:00'
[2025-04-21T13:52:40.694+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-21T13:52:46.467+0000] {scrapping.py:35} INFO - Acessando o link: https://www.farfetch.com/br/shopping/men/items.aspx
[2025-04-21T13:52:53.587+0000] {scrapping.py:54} INFO - Clicou no menu de Jaquetas via JavaScript
[2025-04-21T13:53:01.640+0000] {scrapping.py:76} INFO - Total de produtos: 96
[2025-04-21T13:53:01.672+0000] {logging_mixin.py:190} INFO - Jaqueta x 100 Thieves com capuz
[2025-04-21T13:53:01.680+0000] {logging_mixin.py:190} INFO - adidas
[2025-04-21T13:53:02.681+0000] {scrapping.py:92} INFO - Produto 0 processado
[2025-04-21T13:53:02.716+0000] {logging_mixin.py:190} INFO - Jaqueta Glass Cover-TC
[2025-04-21T13:53:02.723+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:53:03.724+0000] {scrapping.py:92} INFO - Produto 1 processado
[2025-04-21T13:53:03.756+0000] {logging_mixin.py:190} INFO - Jaqueta Ami De Coeur de lã virgem
[2025-04-21T13:53:03.763+0000] {logging_mixin.py:190} INFO - AMI Paris
[2025-04-21T13:53:04.763+0000] {scrapping.py:92} INFO - Produto 2 processado
[2025-04-21T13:53:04.800+0000] {logging_mixin.py:190} INFO - Jaqueta esportiva com listras triplas
[2025-04-21T13:53:04.808+0000] {logging_mixin.py:190} INFO - adidas
[2025-04-21T13:53:05.808+0000] {scrapping.py:92} INFO - Produto 3 processado
[2025-04-21T13:53:05.843+0000] {logging_mixin.py:190} INFO - Jaqueta jeans efeito destroyed
[2025-04-21T13:53:05.850+0000] {logging_mixin.py:190} INFO - Givenchy
[2025-04-21T13:53:06.850+0000] {scrapping.py:92} INFO - Produto 4 processado
[2025-04-21T13:53:06.883+0000] {logging_mixin.py:190} INFO - Jaqueta bomber Reps R-NY
[2025-04-21T13:53:06.889+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:53:07.889+0000] {scrapping.py:92} INFO - Produto 5 processado
[2025-04-21T13:53:07.920+0000] {logging_mixin.py:190} INFO - Jaqueta esportiva Varsity
[2025-04-21T13:53:07.927+0000] {logging_mixin.py:190} INFO - Casablanca
[2025-04-21T13:53:08.928+0000] {scrapping.py:92} INFO - Produto 6 processado
[2025-04-21T13:53:08.959+0000] {logging_mixin.py:190} INFO - Jaqueta de pelos com patch de logo
[2025-04-21T13:53:08.966+0000] {logging_mixin.py:190} INFO - Marni
[2025-04-21T13:53:09.966+0000] {scrapping.py:92} INFO - Produto 7 processado
[2025-04-21T13:53:09.996+0000] {logging_mixin.py:190} INFO - Jaqueta matelassê com patch de bússola
[2025-04-21T13:53:10.002+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:53:11.002+0000] {scrapping.py:92} INFO - Produto 8 processado
[2025-04-21T13:53:11.032+0000] {logging_mixin.py:190} INFO - Jaqueta matelassê camuflada
[2025-04-21T13:53:11.038+0000] {logging_mixin.py:190} INFO - Andersson Bell
[2025-04-21T13:53:12.038+0000] {scrapping.py:92} INFO - Produto 9 processado
[2025-04-21T13:53:12.066+0000] {logging_mixin.py:190} INFO - Jaqueta Retro Denali
[2025-04-21T13:53:12.071+0000] {logging_mixin.py:190} INFO - The North Face
[2025-04-21T13:53:13.072+0000] {scrapping.py:92} INFO - Produto 10 processado
[2025-04-21T13:53:13.103+0000] {logging_mixin.py:190} INFO - Jaqueta matelassê Himalayan com capuz
[2025-04-21T13:53:13.108+0000] {logging_mixin.py:190} INFO - The North Face
[2025-04-21T13:53:14.109+0000] {scrapping.py:92} INFO - Produto 11 processado
[2025-04-21T13:53:14.145+0000] {logging_mixin.py:190} INFO - Blazer Punto com botões
[2025-04-21T13:53:14.150+0000] {logging_mixin.py:190} INFO - Karl Lagerfeld
[2025-04-21T13:53:15.151+0000] {scrapping.py:92} INFO - Produto 12 processado
[2025-04-21T13:53:15.191+0000] {logging_mixin.py:190} INFO - Blazer Beo
[2025-04-21T13:53:15.197+0000] {logging_mixin.py:190} INFO - The Frankie Shop
[2025-04-21T13:50:02.895+0000] {scrapping.py:92} INFO - Produto 13 processado
[2025-04-21T13:53:17.230+0000] {logging_mixin.py:190} INFO - Jaqueta college Mascot
[2025-04-21T13:53:17.236+0000] {logging_mixin.py:190} INFO - Represent
[2025-04-21T13:53:18.237+0000] {scrapping.py:92} INFO - Produto 14 processado
[2025-04-21T13:53:18.268+0000] {logging_mixin.py:190} INFO - Jaqueta matelassê com capuz
[2025-04-21T13:53:18.274+0000] {logging_mixin.py:190} INFO - Karl Lagerfeld Jeans
[2025-04-21T13:53:19.275+0000] {scrapping.py:92} INFO - Produto 15 processado
[2025-04-21T13:53:19.308+0000] {logging_mixin.py:190} INFO - Jaqueta college com bordado
[2025-04-21T13:53:19.314+0000] {logging_mixin.py:190} INFO - BARROW
[2025-04-21T13:53:20.315+0000] {scrapping.py:92} INFO - Produto 16 processado
[2025-04-21T13:53:20.351+0000] {logging_mixin.py:190} INFO - Jaqueta GORE-TEX®
[2025-04-21T13:53:20.358+0000] {logging_mixin.py:190} INFO - The North Face
[2025-04-21T13:53:21.358+0000] {scrapping.py:92} INFO - Produto 17 processado
[2025-04-21T13:53:21.397+0000] {logging_mixin.py:190} INFO - Jaqueta bomber com patch
[2025-04-21T13:53:21.405+0000] {logging_mixin.py:190} INFO - BARROW
[2025-04-21T13:53:22.406+0000] {scrapping.py:92} INFO - Produto 18 processado
[2025-04-21T13:53:22.436+0000] {logging_mixin.py:190} INFO - Jaqueta bomber V-Emblem de couro
[2025-04-21T13:53:22.442+0000] {logging_mixin.py:190} INFO - Versace Jeans Couture
[2025-04-21T13:53:23.442+0000] {scrapping.py:92} INFO - Produto 19 processado
[2025-04-21T13:53:23.473+0000] {logging_mixin.py:190} INFO - Jaqueta impermeável com estampa
[2025-04-21T13:53:23.479+0000] {logging_mixin.py:190} INFO - Kenzo
[2025-04-21T13:53:24.479+0000] {scrapping.py:92} INFO - Produto 20 processado
[2025-04-21T13:53:24.512+0000] {logging_mixin.py:190} INFO - Blazer com abotoamento duplo
[2025-04-21T13:53:24.519+0000] {logging_mixin.py:190} INFO - Karl Lagerfeld
[2025-04-21T13:53:25.520+0000] {scrapping.py:92} INFO - Produto 21 processado
[2025-04-21T13:53:25.550+0000] {logging_mixin.py:190} INFO - Jaqueta com patch de logo
[2025-04-21T13:53:25.556+0000] {logging_mixin.py:190} INFO - Napapijri
[2025-04-21T13:53:26.557+0000] {scrapping.py:92} INFO - Produto 22 processado
[2025-04-21T13:53:26.590+0000] {logging_mixin.py:190} INFO - Jaqueta Essential de bouclê
[2025-04-21T13:53:26.598+0000] {logging_mixin.py:190} INFO - Karl Lagerfeld
[2025-04-21T13:53:27.598+0000] {scrapping.py:92} INFO - Produto 23 processado
[2025-04-21T13:53:27.635+0000] {logging_mixin.py:190} INFO - Jaqueta La Veste Slogan
[2025-04-21T13:53:27.645+0000] {logging_mixin.py:190} INFO - Drôle De Monsieur
[2025-04-21T13:53:28.646+0000] {scrapping.py:92} INFO - Produto 24 processado
[2025-04-21T13:53:28.681+0000] {logging_mixin.py:190} INFO - Colete matelassê com abotoamento duplo
[2025-04-21T13:53:28.687+0000] {logging_mixin.py:190} INFO - Karl Lagerfeld
[2025-04-21T13:53:29.687+0000] {scrapping.py:92} INFO - Produto 25 processado
[2025-04-21T13:53:29.719+0000] {logging_mixin.py:190} INFO - Jaqueta Angler encerada
[2025-04-21T13:53:29.726+0000] {logging_mixin.py:190} INFO - Barbour
[2025-04-21T13:53:30.727+0000] {scrapping.py:92} INFO - Produto 26 processado
[2025-04-21T13:53:30.760+0000] {logging_mixin.py:190} INFO - Jaqueta bomber com detalhe de aplicação
[2025-04-21T13:53:30.767+0000] {logging_mixin.py:190} INFO - Karl Lagerfeld
[2025-04-21T13:53:31.768+0000] {scrapping.py:92} INFO - Produto 27 processado
[2025-04-21T13:53:31.801+0000] {logging_mixin.py:190} INFO - Jaqueta de couro mangas longas
[2025-04-21T13:53:31.807+0000] {logging_mixin.py:190} INFO - Belstaff
[2025-04-21T13:53:32.807+0000] {scrapping.py:92} INFO - Produto 28 processado
[2025-04-21T13:53:32.840+0000] {logging_mixin.py:190} INFO - Jaqueta bomber com zíper
[2025-04-21T13:53:32.847+0000] {logging_mixin.py:190} INFO - Burberry
[2025-04-21T13:53:33.848+0000] {scrapping.py:92} INFO - Produto 29 processado
[2025-04-21T13:53:33.879+0000] {logging_mixin.py:190} INFO - Blazer com abotoamento simples
[2025-04-21T13:53:33.885+0000] {logging_mixin.py:190} INFO - Herno
[2025-04-21T13:53:34.885+0000] {scrapping.py:92} INFO - Produto 30 processado
[2025-04-21T13:53:34.914+0000] {logging_mixin.py:190} INFO - Jaqueta bomber x Wales Bonner dupla face
[2025-04-21T13:53:34.920+0000] {logging_mixin.py:190} INFO - adidas
[2025-04-21T13:53:35.921+0000] {scrapping.py:92} INFO - Produto 31 processado
[2025-04-21T13:53:35.952+0000] {logging_mixin.py:190} INFO - Jaqueta de couro dupla face
[2025-04-21T13:53:35.957+0000] {logging_mixin.py:190} INFO - ROUGH.
[2025-04-21T13:53:36.958+0000] {scrapping.py:92} INFO - Produto 32 processado
[2025-04-21T13:53:36.987+0000] {logging_mixin.py:190} INFO - Jaqueta Glass Cover-TC
[2025-04-21T13:53:36.996+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:53:37.997+0000] {scrapping.py:92} INFO - Produto 33 processado
[2025-04-21T13:53:38.027+0000] {logging_mixin.py:190} INFO - Jaqueta Retro Nuptse 96
[2025-04-21T13:53:38.033+0000] {logging_mixin.py:190} INFO - The North Face
[2025-04-21T13:53:39.034+0000] {scrapping.py:92} INFO - Produto 34 processado
[2025-04-21T13:53:39.061+0000] {logging_mixin.py:190} INFO - Jaqueta com pelos na gola
[2025-04-21T13:53:39.066+0000] {logging_mixin.py:190} INFO - ERALDO
[2025-04-21T13:53:40.067+0000] {scrapping.py:92} INFO - Produto 35 processado
[2025-04-21T13:53:40.093+0000] {logging_mixin.py:190} INFO - Jaqueta bomber com bordado de texto
[2025-04-21T13:53:40.098+0000] {logging_mixin.py:190} INFO - Balmain
[2025-04-21T13:53:41.099+0000] {scrapping.py:92} INFO - Produto 36 processado
[2025-04-21T13:53:41.130+0000] {logging_mixin.py:190} INFO - Jaqueta com estampa Chromo Couture
[2025-04-21T13:53:41.136+0000] {logging_mixin.py:190} INFO - Versace Jeans Couture
[2025-04-21T13:53:42.137+0000] {scrapping.py:92} INFO - Produto 37 processado
[2025-04-21T13:53:42.163+0000] {logging_mixin.py:190} INFO - Colete Pony com logo
[2025-04-21T13:53:42.168+0000] {logging_mixin.py:190} INFO - Polo Ralph Lauren
[2025-04-21T13:53:43.169+0000] {scrapping.py:92} INFO - Produto 38 processado
[2025-04-21T13:53:43.196+0000] {logging_mixin.py:190} INFO - Jaqueta com padronagem monogramada PB
[2025-04-21T13:53:43.201+0000] {logging_mixin.py:190} INFO - Balmain
[2025-04-21T13:53:44.201+0000] {scrapping.py:92} INFO - Produto 39 processado
[2025-04-21T13:53:44.228+0000] {logging_mixin.py:190} INFO - Jaqueta com capuz
[2025-04-21T13:53:44.235+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:53:45.236+0000] {scrapping.py:92} INFO - Produto 40 processado
[2025-04-21T13:53:45.267+0000] {logging_mixin.py:190} INFO - Jaqueta de lã e cashmere mistos
[2025-04-21T13:53:45.273+0000] {logging_mixin.py:190} INFO - AMI Paris
[2025-04-21T13:53:46.273+0000] {scrapping.py:92} INFO - Produto 41 processado
[2025-04-21T13:53:46.302+0000] {logging_mixin.py:190} INFO - Jaqueta flanelada com bolsos
[2025-04-21T13:53:46.308+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:53:48.125+0000] {scrapping.py:92} INFO - Produto 42 processado
[2025-04-21T13:53:48.158+0000] {logging_mixin.py:190} INFO - Jaqueta TNF x Yinka Ilori
[2025-04-21T13:53:48.165+0000] {logging_mixin.py:190} INFO - The North Face
[2025-04-21T13:53:49.166+0000] {scrapping.py:92} INFO - Produto 43 processado
[2025-04-21T13:53:49.204+0000] {logging_mixin.py:190} INFO - Blazer com abotoamento duplo
[2025-04-21T13:53:49.211+0000] {logging_mixin.py:190} INFO - OUR LEGACY
[2025-04-21T13:53:50.211+0000] {scrapping.py:92} INFO - Produto 44 processado
[2025-04-21T13:53:50.242+0000] {logging_mixin.py:190} INFO - Jaqueta de algodão
[2025-04-21T13:53:50.249+0000] {logging_mixin.py:190} INFO - Polo Ralph Lauren
[2025-04-21T13:53:51.250+0000] {scrapping.py:92} INFO - Produto 45 processado
[2025-04-21T13:53:51.282+0000] {logging_mixin.py:190} INFO - Jaqueta corta-vento com logo bordado
[2025-04-21T13:53:51.288+0000] {logging_mixin.py:190} INFO - Fred Perry
[2025-04-21T13:53:52.289+0000] {scrapping.py:92} INFO - Produto 46 processado
[2025-04-21T13:53:52.320+0000] {logging_mixin.py:190} INFO - Jaqueta bomber de couro
[2025-04-21T13:53:52.326+0000] {logging_mixin.py:190} INFO - DSQUARED2
[2025-04-21T13:53:53.326+0000] {scrapping.py:92} INFO - Produto 47 processado
[2025-04-21T13:53:53.357+0000] {logging_mixin.py:190} INFO - Jaqueta bomber com logo
[2025-04-21T13:53:53.363+0000] {logging_mixin.py:190} INFO - Dolce & Gabbana
[2025-04-21T13:53:54.364+0000] {scrapping.py:92} INFO - Produto 48 processado
[2025-04-21T13:53:54.399+0000] {logging_mixin.py:190} INFO - Jaqueta jeans com estampa Tokyo
[2025-04-21T13:53:54.406+0000] {logging_mixin.py:190} INFO - Karl Lagerfeld Jeans
[2025-04-21T13:53:55.406+0000] {scrapping.py:92} INFO - Produto 49 processado
[2025-04-21T13:53:55.437+0000] {logging_mixin.py:190} INFO - Jaqueta bomber Ami De Coeur
[2025-04-21T13:53:55.443+0000] {logging_mixin.py:190} INFO - AMI Paris
[2025-04-21T13:53:56.443+0000] {scrapping.py:92} INFO - Produto 50 processado
[2025-04-21T13:53:56.474+0000] {logging_mixin.py:190} INFO - Jaqueta com capuz e logo de bússola
[2025-04-21T13:53:56.480+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:53:57.480+0000] {scrapping.py:92} INFO - Produto 51 processado
[2025-04-21T13:53:57.511+0000] {logging_mixin.py:190} INFO - Jaqueta Nuptse matelassê 1992
[2025-04-21T13:53:57.517+0000] {logging_mixin.py:190} INFO - The North Face
[2025-04-21T13:53:58.518+0000] {scrapping.py:92} INFO - Produto 52 processado
[2025-04-21T13:53:58.551+0000] {logging_mixin.py:190} INFO - Jaqueta Retro Denali
[2025-04-21T13:53:58.560+0000] {logging_mixin.py:190} INFO - The North Face
[2025-04-21T13:53:59.560+0000] {scrapping.py:92} INFO - Produto 53 processado
[2025-04-21T13:53:59.597+0000] {logging_mixin.py:190} INFO - Colete Retro Denali
[2025-04-21T13:53:59.604+0000] {logging_mixin.py:190} INFO - The North Face
[2025-04-21T13:54:00.604+0000] {scrapping.py:92} INFO - Produto 54 processado
[2025-04-21T13:54:00.636+0000] {logging_mixin.py:190} INFO - Jaqueta com zíper e estampa barroca
[2025-04-21T13:54:00.643+0000] {logging_mixin.py:190} INFO - Versace
[2025-04-21T13:54:01.643+0000] {scrapping.py:92} INFO - Produto 55 processado
[2025-04-21T13:54:01.675+0000] {logging_mixin.py:190} INFO - Jaqueta Pro-Tek matelassê
[2025-04-21T13:54:01.681+0000] {logging_mixin.py:190} INFO - C.P. Company
[2025-04-21T13:54:02.682+0000] {scrapping.py:92} INFO - Produto 56 processado
[2025-04-21T13:54:02.720+0000] {logging_mixin.py:190} INFO - Jaqueta com patch de logo
[2025-04-21T13:54:02.729+0000] {logging_mixin.py:190} INFO - Lacoste
[2025-04-21T13:54:03.729+0000] {scrapping.py:92} INFO - Produto 57 processado
[2025-04-21T13:54:03.763+0000] {logging_mixin.py:190} INFO - Jaqueta com capuz e patch de bússola
[2025-04-21T13:54:03.769+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:54:04.769+0000] {scrapping.py:92} INFO - Produto 58 processado
[2025-04-21T13:54:04.805+0000] {logging_mixin.py:190} INFO - Jaqueta matelassê com aplicação de logo
[2025-04-21T13:54:04.811+0000] {logging_mixin.py:190} INFO - Courrèges
[2025-04-21T13:54:05.812+0000] {scrapping.py:92} INFO - Produto 59 processado
[2025-04-21T13:54:05.843+0000] {logging_mixin.py:190} INFO - Jaqueta com logo bordado
[2025-04-21T13:54:05.849+0000] {logging_mixin.py:190} INFO - Martine Rose
[2025-04-21T13:54:06.850+0000] {scrapping.py:92} INFO - Produto 60 processado
[2025-04-21T13:54:06.886+0000] {logging_mixin.py:190} INFO - Jaqueta college Ivy
[2025-04-21T13:54:06.893+0000] {logging_mixin.py:190} INFO - Axel Arigato
[2025-04-21T13:54:07.894+0000] {scrapping.py:92} INFO - Produto 61 processado
[2025-04-21T13:54:07.931+0000] {logging_mixin.py:190} INFO - Jaqueta Retro Nuptse 1996
[2025-04-21T13:54:07.938+0000] {logging_mixin.py:190} INFO - The North Face
[2025-04-21T13:54:08.938+0000] {scrapping.py:92} INFO - Produto 62 processado
[2025-04-21T13:54:08.971+0000] {logging_mixin.py:190} INFO - Jaqueta jeans efeito destroyed
[2025-04-21T13:54:08.977+0000] {logging_mixin.py:190} INFO - DSQUARED2
[2025-04-21T13:54:09.977+0000] {scrapping.py:92} INFO - Produto 63 processado
[2025-04-21T13:54:10.010+0000] {logging_mixin.py:190} INFO - Jaqueta esportiva com bordado monogramado
[2025-04-21T13:54:10.016+0000] {logging_mixin.py:190} INFO - Palm Angels
[2025-04-21T13:54:11.017+0000] {scrapping.py:92} INFO - Produto 64 processado
[2025-04-21T13:54:11.048+0000] {logging_mixin.py:190} INFO - Jaqueta jeans
[2025-04-21T13:54:11.053+0000] {logging_mixin.py:190} INFO - Saint Laurent
[2025-04-21T13:54:12.054+0000] {scrapping.py:92} INFO - Produto 65 processado
[2025-04-21T13:54:12.082+0000] {logging_mixin.py:190} INFO - Jaqueta matelassê com capuz
[2025-04-21T13:54:12.091+0000] {logging_mixin.py:190} INFO - Mackage
[2025-04-21T13:54:13.092+0000] {scrapping.py:92} INFO - Produto 66 processado
[2025-04-21T13:54:13.121+0000] {logging_mixin.py:190} INFO - Jaqueta bomber com patch de logo
[2025-04-21T13:54:13.127+0000] {logging_mixin.py:190} INFO - Études Studio
[2025-04-21T13:54:14.127+0000] {scrapping.py:92} INFO - Produto 67 processado
[2025-04-21T13:54:14.158+0000] {logging_mixin.py:190} INFO - Jaqueta bomber de couro
[2025-04-21T13:54:14.164+0000] {logging_mixin.py:190} INFO - Dell'oglio
[2025-04-21T13:54:15.164+0000] {scrapping.py:92} INFO - Produto 68 processado
[2025-04-21T13:54:15.196+0000] {logging_mixin.py:190} INFO - Jaqueta com efeito desbotado
[2025-04-21T13:54:15.203+0000] {logging_mixin.py:190} INFO - Balenciaga
[2025-04-21T13:54:16.203+0000] {scrapping.py:92} INFO - Produto 69 processado
[2025-04-21T13:54:16.233+0000] {logging_mixin.py:190} INFO - Jaqueta corta-vento com patch de bússola
[2025-04-21T13:54:16.238+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:54:17.239+0000] {scrapping.py:92} INFO - Produto 70 processado
[2025-04-21T13:54:17.268+0000] {logging_mixin.py:190} INFO - Blazer de linho com abotoamento simples
[2025-04-21T13:54:17.274+0000] {logging_mixin.py:190} INFO - Polo Ralph Lauren
[2025-04-21T13:54:18.911+0000] {scrapping.py:92} INFO - Produto 71 processado
[2025-04-21T13:54:18.943+0000] {logging_mixin.py:190} INFO - Jaqueta com placa de logo
[2025-04-21T13:54:18.949+0000] {logging_mixin.py:190} INFO - Herno
[2025-04-21T13:54:19.950+0000] {scrapping.py:92} INFO - Produto 72 processado
[2025-04-21T13:54:19.989+0000] {logging_mixin.py:190} INFO - Jaqueta Ispa Butterfly
[2025-04-21T13:54:19.997+0000] {logging_mixin.py:190} INFO - Nike
[2025-04-21T13:54:20.997+0000] {scrapping.py:92} INFO - Produto 73 processado
[2025-04-21T13:54:21.029+0000] {logging_mixin.py:190} INFO - Jaqueta bomber matelassê
[2025-04-21T13:54:21.035+0000] {logging_mixin.py:190} INFO - DSQUARED2
[2025-04-21T13:54:22.036+0000] {scrapping.py:92} INFO - Produto 74 processado
[2025-04-21T13:54:22.069+0000] {logging_mixin.py:190} INFO - Jaqueta com patch de bússola
[2025-04-21T13:54:22.076+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:54:23.076+0000] {scrapping.py:92} INFO - Produto 75 processado
[2025-04-21T13:54:23.110+0000] {logging_mixin.py:190} INFO - Colete matelassê
[2025-04-21T13:54:23.117+0000] {logging_mixin.py:190} INFO - Valstar
[2025-04-21T13:54:24.117+0000] {scrapping.py:92} INFO - Produto 76 processado
[2025-04-21T13:54:24.157+0000] {logging_mixin.py:190} INFO - Jaqueta jeans com zíper
[2025-04-21T13:54:24.163+0000] {logging_mixin.py:190} INFO - Off-White
[2025-04-21T13:54:25.163+0000] {scrapping.py:92} INFO - Produto 77 processado
[2025-04-21T13:54:25.196+0000] {logging_mixin.py:190} INFO - Jaqueta jeans de algodão
[2025-04-21T13:54:25.202+0000] {logging_mixin.py:190} INFO - Balenciaga
[2025-04-21T13:54:26.203+0000] {scrapping.py:92} INFO - Produto 78 processado
[2025-04-21T13:54:26.232+0000] {logging_mixin.py:190} INFO - Jaqueta Billy
[2025-04-21T13:54:26.238+0000] {logging_mixin.py:190} INFO - GALLERY DEPT.
[2025-04-21T13:54:27.239+0000] {scrapping.py:92} INFO - Produto 79 processado
[2025-04-21T13:54:27.271+0000] {logging_mixin.py:190} INFO - Colete com detalhe Goggles
[2025-04-21T13:54:27.278+0000] {logging_mixin.py:190} INFO - C.P. Company
[2025-04-21T13:54:28.278+0000] {scrapping.py:92} INFO - Produto 80 processado
[2025-04-21T13:54:28.310+0000] {logging_mixin.py:190} INFO - Jaqueta esportiva com logo bordado
[2025-04-21T13:54:28.316+0000] {logging_mixin.py:190} INFO - Palm Angels
[2025-04-21T13:54:29.317+0000] {scrapping.py:92} INFO - Produto 81 processado
[2025-04-21T13:54:29.348+0000] {logging_mixin.py:190} INFO - Jaqueta Shell-R com capuz
[2025-04-21T13:54:29.355+0000] {logging_mixin.py:190} INFO - C.P. Company
[2025-04-21T13:54:30.355+0000] {scrapping.py:92} INFO - Produto 82 processado
[2025-04-21T13:54:30.392+0000] {logging_mixin.py:190} INFO - Jaqueta matelassê Polo Pony
[2025-04-21T13:54:30.399+0000] {logging_mixin.py:190} INFO - Polo Ralph Lauren
[2025-04-21T13:54:31.399+0000] {scrapping.py:92} INFO - Produto 83 processado
[2025-04-21T13:54:31.429+0000] {logging_mixin.py:190} INFO - Colete Ami de Coeur matelassê
[2025-04-21T13:54:31.435+0000] {logging_mixin.py:190} INFO - AMI Paris
[2025-04-21T13:54:32.436+0000] {scrapping.py:92} INFO - Produto 84 processado
[2025-04-21T13:54:32.474+0000] {logging_mixin.py:190} INFO - Jaqueta bomber de cetim com tag de logo
[2025-04-21T13:54:32.480+0000] {logging_mixin.py:190} INFO - Dolce & Gabbana
[2025-04-21T13:54:33.480+0000] {scrapping.py:92} INFO - Produto 85 processado
[2025-04-21T13:54:33.512+0000] {logging_mixin.py:190} INFO - Jaqueta com patch de bússola
[2025-04-21T13:54:33.517+0000] {logging_mixin.py:190} INFO - Stone Island
[2025-04-21T13:54:34.518+0000] {scrapping.py:92} INFO - Produto 86 processado
[2025-04-21T13:54:34.549+0000] {logging_mixin.py:190} INFO - Colete Garson matelassê
[2025-04-21T13:54:34.555+0000] {logging_mixin.py:190} INFO - Canada Goose
[2025-04-21T13:54:35.556+0000] {scrapping.py:92} INFO - Produto 87 processado
[2025-04-21T13:54:35.586+0000] {logging_mixin.py:190} INFO - Jaqueta esportiva color block
[2025-04-21T13:54:35.594+0000] {logging_mixin.py:190} INFO - Moschino
[2025-04-21T13:54:36.594+0000] {scrapping.py:92} INFO - Produto 88 processado
[2025-04-21T13:54:36.628+0000] {logging_mixin.py:190} INFO - Jaqueta com capuz e zíper
[2025-04-21T13:54:36.634+0000] {logging_mixin.py:190} INFO - Balenciaga
[2025-04-21T13:54:37.634+0000] {scrapping.py:92} INFO - Produto 89 processado
[2025-04-21T13:54:37.662+0000] {logging_mixin.py:190} INFO - Colete Gorham
[2025-04-21T13:54:37.668+0000] {logging_mixin.py:190} INFO - Polo Ralph Lauren
[2025-04-21T13:54:38.668+0000] {scrapping.py:92} INFO - Produto 90 processado
[2025-04-21T13:54:38.701+0000] {logging_mixin.py:190} INFO - Colete com detalhe Goggles
[2025-04-21T13:54:38.709+0000] {logging_mixin.py:190} INFO - C.P. Company
[2025-04-21T13:54:39.710+0000] {scrapping.py:92} INFO - Produto 91 processado
[2025-04-21T13:54:39.738+0000] {logging_mixin.py:190} INFO - Jaqueta college de veludo cotelê com recortes
[2025-04-21T13:54:39.744+0000] {logging_mixin.py:190} INFO - AMIRI
[2025-04-21T13:54:40.744+0000] {scrapping.py:92} INFO - Produto 92 processado
[2025-04-21T13:54:40.775+0000] {logging_mixin.py:190} INFO - Jaqueta de couro
[2025-04-21T13:54:40.782+0000] {logging_mixin.py:190} INFO - Salvatore Santoro
[2025-04-21T13:54:41.782+0000] {scrapping.py:92} INFO - Produto 93 processado
[2025-04-21T13:54:41.811+0000] {logging_mixin.py:190} INFO - Jaqueta jeans com logo bordado
[2025-04-21T13:54:41.817+0000] {logging_mixin.py:190} INFO - Who Decides War
[2025-04-21T13:54:42.818+0000] {scrapping.py:92} INFO - Produto 94 processado
[2025-04-21T13:54:42.846+0000] {logging_mixin.py:190} INFO - Jaqueta com logo gravado
[2025-04-21T13:54:42.852+0000] {logging_mixin.py:190} INFO - Emporio Armani
[2025-04-21T13:54:43.853+0000] {scrapping.py:92} INFO - Produto 95 processado
[2025-04-21T13:54:47.414+0000] {scrapping.py:121} INFO - Dados salvos em: /opt/***/dados/bruto/promocoes_jaquetas_21042025_1.csv
[2025-04-21T13:54:47.414+0000] {scrapping.py:123} INFO - Iniciando tratamento...
[2025-04-21T13:54:47.424+0000] {tratamento.py:34} INFO - Arquivo mais recente encontrado: /opt/***/dados/bruto/promocoes_jaquetas_21042025_1.csv
[2025-04-21T13:54:56.783+0000] {logging_mixin.py:190} INFO - +--------------------+--------------------+-----+--------------+-------------------+--------+--------------+-----+-------------------+-------------+
|             Produto|               Marca|Preço|Preço Original|               Data|Desconto|Categoria Luxo|Media|Desconto Percentual|Classificação|
+--------------------+--------------------+-----+--------------+-------------------+--------+--------------+-----+-------------------+-------------+
|Jaqueta x 100 Thi...|              adidas| 3734|          3895|21-04-2025 13:54:55|  0.0413|        Luxoso|Baixa|               4.13|      Jaqueta|
|Jaqueta Glass Cov...|        Stone Island|19542|         26205|21-04-2025 13:54:55|  0.2543| Muito Luxuoso| Alta|              25.43|      Jaqueta|
|Jaqueta Ami De Co...|           AMI Paris| 6446|          7478|21-04-2025 13:54:55|   0.138|        Luxoso|Baixa|               13.8|      Jaqueta|
|Jaqueta esportiva...|              adidas| 1524|          3036|21-04-2025 13:54:55|   0.498|  Pouco Luxoso|Baixa|               49.8|      Jaqueta|
|Jaqueta jeans efe...|            Givenchy|21805|         30121|21-04-2025 13:54:55|  0.2761| Muito Luxuoso| Alta|              27.61|      Jaqueta|
|Jaqueta bomber Re...|        Stone Island| 8046|          9499|21-04-2025 13:54:55|   0.153|        Luxoso| Alta|               15.3|      Jaqueta|
|Jaqueta esportiva...|          Casablanca|12171|         15323|21-04-2025 13:54:55|  0.2057| Muito Luxuoso| Alta|              20.57|      Jaqueta|
|Jaqueta de pelos ...|               Marni|13044|         17594|21-04-2025 13:54:55|  0.2586| Muito Luxuoso| Alta|              25.86|      Jaqueta|
|Jaqueta matelassê...|        Stone Island| 7777|          8639|21-04-2025 13:54:55|  0.0998|        Luxoso|Baixa|               9.98|      Jaqueta|
|Jaqueta matelassê...|      Andersson Bell| 3789|          8405|21-04-2025 13:54:55|  0.5492|        Luxoso|Baixa|              54.92|      Jaqueta|
|Jaqueta Retro Denali|      The North Face| 1629|          2322|21-04-2025 13:54:55|  0.2984|  Pouco Luxoso|Baixa|              29.84|      Jaqueta|
|Jaqueta matelassê...|      The North Face| 3870|          5364|21-04-2025 13:54:55|  0.2785|        Luxoso|Baixa|              27.85|      Jaqueta|
|Blazer Punto com ...|      Karl Lagerfeld| 2567|          5122|21-04-2025 13:54:55|  0.4988|        Luxoso|Baixa|              49.88|       Blazer|
|          Blazer Beo|    The Frankie Shop| 2180|          3625|21-04-2025 13:54:55|  0.3986|        Luxoso|Baixa|              39.86|       Blazer|
|Jaqueta college M...|           Represent| 4223|          6028|21-04-2025 13:54:55|  0.2994|        Luxoso|Baixa|              29.94|      Jaqueta|
|Jaqueta matelassê...|Karl Lagerfeld Jeans| 2055|          4097|21-04-2025 13:54:55|  0.4984|        Luxoso|Baixa|              49.84|      Jaqueta|
|Jaqueta college c...|              BARROW| 3334|          6655|21-04-2025 13:54:55|   0.499|        Luxoso|Baixa|               49.9|      Jaqueta|
|   Jaqueta GORE-TEX®|      The North Face| 7689|          9911|21-04-2025 13:54:55|  0.2242|        Luxoso|Baixa|              22.42|      Jaqueta|
|Jaqueta bomber co...|              BARROW| 3334|          6655|21-04-2025 13:54:55|   0.499|        Luxoso|Baixa|               49.9|      Jaqueta|
|Jaqueta bomber V-...|Versace Jeans Cou...| 6873|          9813|21-04-2025 13:54:55|  0.2996|        Luxoso|Baixa|              29.96|      Jaqueta|
+--------------------+--------------------+-----+--------------+-------------------+--------+--------------+-----+-------------------+-------------+
only showing top 20 rows
[2025-04-21T13:54:57.444+0000] {tratamento.py:94} INFO - CSV tratado salvo como jaquetas_tratado.csv
[2025-04-21T13:55:03.560+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/scrapping.py", line 127, in run_scraping
    salvar_dados(produtos_lista)
  File "/opt/airflow/scrapping.py", line 124, in salvar_dados
    tratamento()
  File "/opt/airflow/tratamento.py", line 107, in tratamento
    df_completo.coalesce(1).write.mode("overwrite").parquet(f"file://{path_parquet}")
  File "/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 885, in parquet
    self._jwrite.parquet(path)
  File "/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o110.parquet.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 7) (32047fb085ea executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/opt/airflow/dados/tratado/parquet.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkFileNotFoundException: File file:/opt/airflow/dados/tratado/parquet/dados_tratado.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
	... 17 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/opt/airflow/dados/tratado/parquet.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.SparkFileNotFoundException: File file:/opt/airflow/dados/tratado/parquet/dados_tratado.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
	... 17 more

[2025-04-21T13:55:03.629+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=scraping_farfetch_mensal, task_id=executar_scraping_farfetch, run_id=manual__2025-04-21T13:49:50.948276+00:00, execution_date=20250421T134950, start_date=20250421T135240, end_date=20250421T135503
[2025-04-21T13:55:03.670+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-21T13:55:03.671+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 3 for task executar_scraping_farfetch (An error occurred while calling o110.parquet.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 7) (32047fb085ea executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/opt/airflow/dados/tratado/parquet.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkFileNotFoundException: File file:/opt/airflow/dados/tratado/parquet/dados_tratado.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
	... 17 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/opt/airflow/dados/tratado/parquet.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.SparkFileNotFoundException: File file:/opt/airflow/dados/tratado/parquet/dados_tratado.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
	... 17 more
; 1349)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/scrapping.py", line 127, in run_scraping
    salvar_dados(produtos_lista)
  File "/opt/airflow/scrapping.py", line 124, in salvar_dados
    tratamento()
  File "/opt/airflow/tratamento.py", line 107, in tratamento
    df_completo.coalesce(1).write.mode("overwrite").parquet(f"file://{path_parquet}")
  File "/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 885, in parquet
    self._jwrite.parquet(path)
  File "/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o110.parquet.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 7) (32047fb085ea executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/opt/airflow/dados/tratado/parquet.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkFileNotFoundException: File file:/opt/airflow/dados/tratado/parquet/dados_tratado.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
	... 17 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/opt/airflow/dados/tratado/parquet.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.SparkFileNotFoundException: File file:/opt/airflow/dados/tratado/parquet/dados_tratado.parquet does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
	... 17 more

[2025-04-21T13:55:03.838+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-21T13:55:03.861+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-21T13:55:03.866+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
